{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6b4B2ki8dVaMuP9OHz9bm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshatha-21/harshathaeng/blob/main/health_care_qnd_a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "MmRVm53NeDTg",
        "outputId": "576ced94-9f66-48a0-cfbf-c1f2bb722d26"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'true' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f6aa3210337c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0;34m\"provenance\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;34m\"authorship_tag\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ABX9TyP+tXsKISH4UQpl8fGEkp3z\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0;34m\"include_colab_link\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     },\n\u001b[1;32m     10\u001b[0m     \"kernelspec\": {\n",
            "\u001b[0;31mNameError\u001b[0m: name 'true' is not defined"
          ]
        }
      ],
      "source": [
        "{\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"colab\": {\n",
        "      \"provenance\": [],\n",
        "      \"authorship_tag\": \"ABX9TyP+tXsKISH4UQpl8fGEkp3z\",\n",
        "      \"include_colab_link\": true\n",
        "    },\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    }\n",
        "  },\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"markdown\",\n",
        "      \"metadata\": {\n",
        "        \"id\": \"view-in-github\",\n",
        "        \"colab_type\": \"text\"\n",
        "      },\n",
        "      \"source\": [\n",
        "        \"<a href=\\\"https://colab.research.google.com/github/Kalyanianikkath/NM-Genrative-AI/blob/main/healthcare_Q%26A.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"execution_count\": 2,\n",
        "      \"metadata\": {\n",
        "        \"colab\": {\n",
        "          \"base_uri\": \"https://localhost:8080/\",\n",
        "          \"height\": 1000\n",
        "        },\n",
        "        \"id\": \"Qg6D3qrZooUF\",\n",
        "        \"outputId\": \"1dd0d2dc-55db-4496-b86b-78894f6c6f05\"\n",
        "      },\n",
        "      \"outputs\": [\n",
        "        {\n",
        "          \"output_type\": \"stream\",\n",
        "          \"name\": \"stdout\",\n",
        "          \"text\": [\n",
        "            \"Collecting langchain-community\\n\",\n",
        "            \"  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\\n\",\n",
        "            \"Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.51)\\n\",\n",
        "            \"Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\\n\",\n",
        "            \"Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\\n\",\n",
        "            \"Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\\n\",\n",
        "            \"Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\\n\",\n",
        "            \"Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\\n\",\n",
        "            \"Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\\n\",\n",
        "            \"Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\\n\",\n",
        "            \"  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\\n\",\n",
        "            \"Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\\n\",\n",
        "            \"  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\\n\",\n",
        "            \"Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.28)\\n\",\n",
        "            \"Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\\n\",\n",
        "            \"  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\\n\",\n",
        "            \"Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\\n\",\n",
        "            \"Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\\n\",\n",
        "            \"Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\\n\",\n",
        "            \"Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\\n\",\n",
        "            \"Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\\n\",\n",
        "            \"Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.2)\\n\",\n",
        "            \"Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\\n\",\n",
        "            \"Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\\n\",\n",
        "            \"Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\\n\",\n",
        "            \"  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\\n\",\n",
        "            \"Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\\n\",\n",
        "            \"  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\\n\",\n",
        "            \"Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\\n\",\n",
        "            \"Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\\n\",\n",
        "            \"Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\\n\",\n",
        "            \"Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\\n\",\n",
        "            \"Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.1)\\n\",\n",
        "            \"Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\\n\",\n",
        "            \"Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\\n\",\n",
        "            \"Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\\n\",\n",
        "            \"Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\\n\",\n",
        "            \"Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\\n\",\n",
        "            \"  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\\n\",\n",
        "            \"Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\\n\",\n",
        "            \"Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\\n\",\n",
        "            \"Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\\n\",\n",
        "            \"Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\\n\",\n",
        "            \"Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\\n\",\n",
        "            \"Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\\n\",\n",
        "            \"Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\\n\",\n",
        "            \"Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\\n\",\n",
        "            \"Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\\n\",\n",
        "            \"Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\\n\",\n",
        "            \"Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\\n\",\n",
        "            \"Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.4.0)\\n\",\n",
        "            \"Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\\n\",\n",
        "            \"  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\\n\",\n",
        "            \"Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\\n\",\n",
        "            \"Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\\n\",\n",
        "            \"\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m2.5/2.5 MB\\u001b[0m \\u001b[31m51.5 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n\",\n",
        "            \"\\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\\n\",\n",
        "            \"Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\\n\",\n",
        "            \"Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\\n\",\n",
        "            \"Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\\n\",\n",
        "            \"\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m50.9/50.9 kB\\u001b[0m \\u001b[31m4.8 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n\",\n",
        "            \"\\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\\n\",\n",
        "            \"Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\\n\",\n",
        "            \"Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\\n\",\n",
        "            \"Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\\n\",\n",
        "            \"Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0\\n\"\n",
        "          ]\n",
        "        },\n",
        "        {\n",
        "          \"output_type\": \"error\",\n",
        "          \"ename\": \"ImportError\",\n",
        "          \"evalue\": \"cannot import name 'GoogleGenerativeAIEmbeddings' from 'langchain.embeddings' (/usr/local/lib/python3.11/dist-packages/langchain/embeddings/__init__.py)\",\n",
        "          \"traceback\": [\n",
        "            \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\n",
        "            \"\\u001b[0;31mImportError\\u001b[0m                               Traceback (most recent call last)\",\n",
        "            \"\\u001b[0;32m<ipython-input-2-c064b74c7d06>\\u001b[0m in \\u001b[0;36m<cell line: 0>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      6\\u001b[0m \\u001b[0;32mimport\\u001b[0m \\u001b[0mos\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      7\\u001b[0m \\u001b[0;32mfrom\\u001b[0m \\u001b[0mlangchain\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mvectorstores\\u001b[0m \\u001b[0;32mimport\\u001b[0m \\u001b[0mFAISS\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 8\\u001b[0;31m \\u001b[0;32mfrom\\u001b[0m \\u001b[0mlangchain\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0membeddings\\u001b[0m \\u001b[0;32mimport\\u001b[0m \\u001b[0mGoogleGenerativeAIEmbeddings\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      9\\u001b[0m \\u001b[0;32mfrom\\u001b[0m \\u001b[0mlangchain\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mtext_splitter\\u001b[0m \\u001b[0;32mimport\\u001b[0m \\u001b[0mRecursiveCharacterTextSplitter\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m     10\\u001b[0m \\u001b[0;32mfrom\\u001b[0m \\u001b[0mlangchain\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mchains\\u001b[0m \\u001b[0;32mimport\\u001b[0m \\u001b[0mRetrievalQA\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n",
        "            \"\\u001b[0;31mImportError\\u001b[0m: cannot import name 'GoogleGenerativeAIEmbeddings' from 'langchain.embeddings' (/usr/local/lib/python3.11/dist-packages/langchain/embeddings/__init__.py)\",\n",
        "            \"\",\n",
        "            \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0;32m\\nNOTE: If your import is failing due to a missing package, you can\\nmanually install dependencies using either !pip or !apt.\\n\\nTo view examples of installing some common dependencies, click the\\n\\\"Open Examples\\\" button below.\\n\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\\n\"\n",
        "          ],\n",
        "          \"errorDetails\": {\n",
        "            \"actions\": [\n",
        "              {\n",
        "                \"action\": \"open_url\",\n",
        "                \"actionText\": \"Open Examples\",\n",
        "                \"url\": \"/notebooks/snippets/importing_libraries.ipynb\"\n",
        "              }\n",
        "            ]\n",
        "          }\n",
        "        }\n",
        "      ],\n",
        "      \"source\": [\n",
        "        \"# ✅ Step 1: Install dependencies\\n\",\n",
        "        \"!pip install -q langchain faiss-cpu langchain-google-genai biopython google-generativeai\\n\",\n",
        "        \"!pip install -U langchain-community\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 2: Import necessary libraries\\n\",\n",
        "        \"import os\\n\",\n",
        "        \"from langchain.vectorstores import FAISS\\n\",\n",
        "        \"from langchain.embeddings import GoogleGenerativeAIEmbeddings\\n\",\n",
        "        \"from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\",\n",
        "        \"from langchain.chains import RetrievalQA\\n\",\n",
        "        \"from langchain_google_genai import ChatGoogleGenerativeAI\\n\",\n",
        "        \"from Bio import Entrez\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 3: API keys\\n\",\n",
        "        \"os.environ[\\\"GOOGLE_API_KEY\\\"] = \\\"AIzaSyCW5HzSyVUTqh7VbS6nFUiJsoo2fKSjbwM\\\"  # Replace this with your Gemini API key\\n\",\n",
        "        \"Entrez.email = \\\"kalyaniaanikkath@gmail.com\\\"  # Replace this with your email for PubMed API\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 4: Fetch articles from PubMed\\n\",\n",
        "        \"def fetch_pubmed_articles(query, max_results=5):\\n\",\n",
        "        \"    handle = Entrez.esearch(db=\\\"pubmed\\\", term=query, retmax=max_results)\\n\",\n",
        "        \"    record = Entrez.read(handle)\\n\",\n",
        "        \"    ids = record[\\\"IdList\\\"]\\n\",\n",
        "        \"    abstracts = []\\n\",\n",
        "        \"    for pmid in ids:\\n\",\n",
        "        \"        fetch = Entrez.efetch(db=\\\"pubmed\\\", id=pmid, rettype=\\\"abstract\\\", retmode=\\\"text\\\")\\n\",\n",
        "        \"        abstract_text = fetch.read()\\n\",\n",
        "        \"        abstracts.append(abstract_text)\\n\",\n",
        "        \"    return abstracts\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 5: Build vector store from articles\\n\",\n",
        "        \"def build_vectorstore_from_articles(articles):\\n\",\n",
        "        \"    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\n\",\n",
        "        \"    texts = text_splitter.create_documents(articles)\\n\",\n",
        "        \"    embeddings = GoogleGenerativeAIEmbeddings(model=\\\"models/embedding-001\\\")\\n\",\n",
        "        \"    vectorstore = FAISS.from_documents(texts, embeddings)\\n\",\n",
        "        \"    return vectorstore\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 6: Create QA chain\\n\",\n",
        "        \"def create_qa_chain(vectorstore):\\n\",\n",
        "        \"    llm = ChatGoogleGenerativeAI(model=\\\"gemini-pro\\\", temperature=0.2)\\n\",\n",
        "        \"    retriever = vectorstore.as_retriever()\\n\",\n",
        "        \"    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\\n\",\n",
        "        \"    return qa_chain\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 7: Ask a health question\\n\",\n",
        "        \"def ask_health_question(query, qa_chain):\\n\",\n",
        "        \"    result = qa_chain(query)\\n\",\n",
        "        \"    print(\\\"🩺 Answer:\\\\n\\\")\\n\",\n",
        "        \"    print(result[\\\"result\\\"])\\n\",\n",
        "        \"    print(\\\"\\\\n📚 Sources:\\\")\\n\",\n",
        "        \"    for i, doc in enumerate(result[\\\"source_documents\\\"]):\\n\",\n",
        "        \"        print(f\\\"\\\\nSource {i+1}:\\\\n{doc.page_content[:500]}...\\\")  # show a snippet\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 8: Run the chatbot\\n\",\n",
        "        \"if __name__ == \\\"__main__\\\":\\n\",\n",
        "        \"    user_query = \\\"What are the latest treatments for type 2 diabetes?\\\"  # ← You can change the question\\n\",\n",
        "        \"    print(\\\"🔍 Searching PubMed...\\\")\\n\",\n",
        "        \"    articles = fetch_pubmed_articles(user_query, max_results=5)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    print(\\\"📚 Building vector store...\\\")\\n\",\n",
        "        \"    vectorstore = build_vectorstore_from_articles(articles)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    print(\\\"🤖 Initializing Q&A chain with Gemini...\\\")\\n\",\n",
        "        \"    qa_chain = create_qa_chain(vectorstore)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    print(\\\"\\\\n💬 Asking your question...\\\\n\\\")\\n\",\n",
        "        \"    ask_health_question(user_query, qa_chain)\\n\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"# ✅ Step 1: Install dependencies\\n\",\n",
        "        \"!pip install -q langchain faiss-cpu langchain-google-genai biopython google-generativeai\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 2: Import necessary libraries\\n\",\n",
        "        \"import os\\n\",\n",
        "        \"from langchain.vectorstores import FAISS\\n\",\n",
        "        \"from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\",\n",
        "        \"from langchain.chains import RetrievalQA\\n\",\n",
        "        \"from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\\n\",\n",
        "        \"from Bio import Entrez\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 3: API keys\\n\",\n",
        "        \"os.environ[\\\"GOOGLE_API_KEY\\\"] = \\\"AIzaSyCW5HzSyVUTqh7VbS6nFUiJsoo2fKSjbwM\\\"  # Replace this with your Gemini API key\\n\",\n",
        "        \"Entrez.email = \\\"kalyaniaanikkath@gmail.com\\\"  # Replace this with your email for PubMed API\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 4: Fetch articles from PubMed\\n\",\n",
        "        \"def fetch_pubmed_articles(query, max_results=5):\\n\",\n",
        "        \"    handle = Entrez.esearch(db=\\\"pubmed\\\", term=query, retmax=max_results)\\n\",\n",
        "        \"    record = Entrez.read(handle)\\n\",\n",
        "        \"    ids = record[\\\"IdList\\\"]\\n\",\n",
        "        \"    abstracts = []\\n\",\n",
        "        \"    for pmid in ids:\\n\",\n",
        "        \"        fetch = Entrez.efetch(db=\\\"pubmed\\\", id=pmid, rettype=\\\"abstract\\\", retmode=\\\"text\\\")\\n\",\n",
        "        \"        abstract_text = fetch.read()\\n\",\n",
        "        \"        abstracts.append(abstract_text)\\n\",\n",
        "        \"    return abstracts\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 5: Build vector store from articles\\n\",\n",
        "        \"def build_vectorstore_from_articles(articles):\\n\",\n",
        "        \"    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\n\",\n",
        "        \"    texts = text_splitter.create_documents(articles)\\n\",\n",
        "        \"    embeddings = GoogleGenerativeAIEmbeddings(model=\\\"models/embedding-001\\\")\\n\",\n",
        "        \"    vectorstore = FAISS.from_documents(texts, embeddings)\\n\",\n",
        "        \"    return vectorstore\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 6: Create QA chain\\n\",\n",
        "        \"def create_qa_chain(vectorstore):\\n\",\n",
        "        \"    llm = ChatGoogleGenerativeAI(model=\\\"gemini-1.5-pro\\\", temperature=0.2)\\n\",\n",
        "        \"    retriever = vectorstore.as_retriever()\\n\",\n",
        "        \"    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\\n\",\n",
        "        \"    return qa_chain\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 7: Ask a health question\\n\",\n",
        "        \"def ask_health_question(query, qa_chain):\\n\",\n",
        "        \"    result = qa_chain(query)\\n\",\n",
        "        \"    print(\\\"🩺 Answer:\\\\n\\\")\\n\",\n",
        "        \"    print(result[\\\"result\\\"])\\n\",\n",
        "        \"    print(\\\"\\\\n📚 Sources:\\\")\\n\",\n",
        "        \"    for i, doc in enumerate(result[\\\"source_documents\\\"]):\\n\",\n",
        "        \"        print(f\\\"\\\\nSource {i+1}:\\\\n{doc.page_content[:500]}...\\\")  # show a snippet\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 8: Run the chatbot\\n\",\n",
        "        \"if __name__ == \\\"__main__\\\":\\n\",\n",
        "        \"    user_query = \\\"What are the latest treatments for type 2 diabetes?\\\"  # ← You can change the question\\n\",\n",
        "        \"    print(\\\"🔍 Searching PubMed...\\\")\\n\",\n",
        "        \"    articles = fetch_pubmed_articles(user_query, max_results=5)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    print(\\\"📚 Building vector store...\\\")\\n\",\n",
        "        \"    vectorstore = build_vectorstore_from_articles(articles)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    print(\\\"🤖 Initializing Q&A chain with Gemini...\\\")\\n\",\n",
        "        \"    qa_chain = create_qa_chain(vectorstore)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    print(\\\"\\\\n💬 Asking your question...\\\\n\\\")\\n\",\n",
        "        \"    ask_health_question(user_query, qa_chain)\\n\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"colab\": {\n",
        "          \"base_uri\": \"https://localhost:8080/\"\n",
        "        },\n",
        "        \"id\": \"TlQHY82bvWX4\",\n",
        "        \"outputId\": \"f5d5e570-9827-484b-cd71-f81bf548b445\"\n",
        "      },\n",
        "      \"execution_count\": 5,\n",
        "      \"outputs\": [\n",
        "        {\n",
        "          \"output_type\": \"stream\",\n",
        "          \"name\": \"stdout\",\n",
        "          \"text\": [\n",
        "            \"🔍 Searching PubMed...\\n\",\n",
        "            \"📚 Building vector store...\\n\",\n",
        "            \"🤖 Initializing Q&A chain with Gemini...\\n\",\n",
        "            \"\\n\",\n",
        "            \"💬 Asking your question...\\n\",\n",
        "            \"\\n\",\n",
        "            \"🩺 Answer:\\n\",\n",
        "            \"\\n\",\n",
        "            \"Based on the provided texts, flash glucose monitoring and adherence to the 2023 ADA Standards of Care, including a diabetes care bundle, are discussed as treatments and management strategies for type 2 diabetes.  Specific medications within these guidelines are not detailed in this text.\\n\",\n",
        "            \"\\n\",\n",
        "            \"📚 Sources:\\n\",\n",
        "            \"\\n\",\n",
        "            \"Source 1:\\n\",\n",
        "            \"FSL for 24 months' follow-up. HCRU was measured for 12 months before the index \\n\",\n",
        "            \"date and the last 12 months of the 24-month follow-up period. HbA1c data were \\n\",\n",
        "            \"taken from the latest tests in each period.\\n\",\n",
        "            \"RESULTS: Mean HbA1c was statistically significantly reduced after FSL \\n\",\n",
        "            \"acquisition among people aged ≤65 or >65 years in all four treatment groups \\n\",\n",
        "            \"(range, 0.3-0.8% reduction). After FSL acquisition, ED visits and \\n\",\n",
        "            \"hospitalization were statistically significantly reduced in the oral therapy \\n\",\n",
        "            \"only...\\n\",\n",
        "            \"\\n\",\n",
        "            \"Source 2:\\n\",\n",
        "            \"Promoting provider adherence to American Diabetes Association guidelines with a \\n\",\n",
        "            \"diabetes care bundle: A DNP quality improvement project.\\n\",\n",
        "            \"\\n\",\n",
        "            \"Noor B, Benton K, Vazquez C.\\n\",\n",
        "            \"\\n\",\n",
        "            \"Type 2 diabetes mellitus (T2DM) is a common and expensive health condition. \\n\",\n",
        "            \"Patients are at increased risk for cardiorenal complications when metabolic \\n\",\n",
        "            \"targets for hemoglobin A1C, BP, and low-density lipoprotein cholesterol are \\n\",\n",
        "            \"unmet. Many providers do not fully adhere to the latest diabetes guidelines. \\n\",\n",
        "            \"This quality improveme...\\n\",\n",
        "            \"\\n\",\n",
        "            \"Source 3:\\n\",\n",
        "            \"Author information:\\n\",\n",
        "            \"(1)Professional Degree Program, School of Public Health, Graduate School of \\n\",\n",
        "            \"Medicine, The University of Tokyo, Tokyo, Japan.\\n\",\n",
        "            \"(2)Department of Biomedical Informatics, Graduate School of Medicine, The \\n\",\n",
        "            \"University of Tokyo, Tokyo, Japan.\\n\",\n",
        "            \"(3)Department of Diabetes and Metabolic Diseases, Graduate School of Medicine, \\n\",\n",
        "            \"The University of Tokyo, Tokyo, Japan.\\n\",\n",
        "            \"(4)Division of Diabetes, Mitsui Memorial Hospital, Tokyo, Japan.\\n\",\n",
        "            \"(5)Department of Metabolism and Endocrinology, Akita Univers...\\n\",\n",
        "            \"\\n\",\n",
        "            \"Source 4:\\n\",\n",
        "            \"Copyright © 2025 Wolters Kluwer Health, Inc. All rights reserved.\\n\",\n",
        "            \"\\n\",\n",
        "            \"DOI: 10.1097/01.NPR.0000000000000300\\n\",\n",
        "            \"PMID: 40128208 [Indexed for MEDLINE]...\\n\"\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"# ✅ Step 1: Install dependencies\\n\",\n",
        "        \"!pip install -q langchain faiss-cpu langchain-google-genai biopython google-generativeai\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 2: Import necessary libraries\\n\",\n",
        "        \"import os\\n\",\n",
        "        \"from langchain.vectorstores import FAISS\\n\",\n",
        "        \"from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\",\n",
        "        \"from langchain.chains import RetrievalQA\\n\",\n",
        "        \"from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\\n\",\n",
        "        \"from Bio import Entrez\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 3: API keys\\n\",\n",
        "        \"os.environ[\\\"GOOGLE_API_KEY\\\"] = \\\"your_gemini_api_key_here\\\"  # Replace with your Gemini API key\\n\",\n",
        "        \"Entrez.email = \\\"your_email@example.com\\\"  # Replace with your email for PubMed\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 4: Fetch articles from PubMed\\n\",\n",
        "        \"def fetch_pubmed_articles(query, max_results=5):\\n\",\n",
        "        \"    handle = Entrez.esearch(db=\\\"pubmed\\\", term=query, retmax=max_results)\\n\",\n",
        "        \"    record = Entrez.read(handle)\\n\",\n",
        "        \"    ids = record[\\\"IdList\\\"]\\n\",\n",
        "        \"    abstracts = []\\n\",\n",
        "        \"    for pmid in ids:\\n\",\n",
        "        \"        fetch = Entrez.efetch(db\\n\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"colab\": {\n",
        "          \"base_uri\": \"https://localhost:8080/\",\n",
        "          \"height\": 108\n",
        "        },\n",
        "        \"id\": \"o4C5FPiVwIYm\",\n",
        "        \"outputId\": \"c8f5edfd-7370-4480-d125-9794f91578a4\"\n",
        "      },\n",
        "      \"execution_count\": 10,\n",
        "      \"outputs\": [\n",
        "        {\n",
        "          \"output_type\": \"error\",\n",
        "          \"ename\": \"SyntaxError\",\n",
        "          \"evalue\": \"incomplete input (<ipython-input-10-94ae1912d123>, line 23)\",\n",
        "          \"traceback\": [\n",
        "            \"\\u001b[0;36m  File \\u001b[0;32m\\\"<ipython-input-10-94ae1912d123>\\\"\\u001b[0;36m, line \\u001b[0;32m23\\u001b[0m\\n\\u001b[0;31m    fetch = Entrez.efetch(db\\u001b[0m\\n\\u001b[0m                            ^\\u001b[0m\\n\\u001b[0;31mSyntaxError\\u001b[0m\\u001b[0;31m:\\u001b[0m incomplete input\\n\"\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"# ✅ Step 1: Install dependencies\\n\",\n",
        "        \"!pip install -q langchain faiss-cpu langchain-google-genai biopython google-generativeai\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 2: Import necessary libraries\\n\",\n",
        "        \"import os\\n\",\n",
        "        \"from langchain.vectorstores import FAISS\\n\",\n",
        "        \"from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\",\n",
        "        \"from langchain.chains import RetrievalQA\\n\",\n",
        "        \"from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\\n\",\n",
        "        \"from Bio import Entrez\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 3: API keys\\n\",\n",
        "        \"os.environ[\\\"GOOGLE_API_KEY\\\"] = \\\"AIzaSyCW5HzSyVUTqh7VbS6nFUiJsoo2fKSjbwM\\\"  # Replace with your Gemini API key\\n\",\n",
        "        \"Entrez.email = \\\"kalyaniaanikkath@gmail.com\\\"  # Replace with your email for PubMed\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 4: Fetch articles from PubMed\\n\",\n",
        "        \"def fetch_pubmed_articles(query, max_results=5):\\n\",\n",
        "        \"    handle = Entrez.esearch(db=\\\"pubmed\\\", term=query, retmax=max_results)\\n\",\n",
        "        \"    record = Entrez.read(handle)\\n\",\n",
        "        \"    ids = record[\\\"IdList\\\"]\\n\",\n",
        "        \"    abstracts = []\\n\",\n",
        "        \"    for pmid in ids:\\n\",\n",
        "        \"        fetch = Entrez.efetch(db=\\\"pubmed\\\", id=pmid, rettype=\\\"abstract\\\", retmode=\\\"text\\\")\\n\",\n",
        "        \"        abstract_text = fetch.read()\\n\",\n",
        "        \"        abstracts.append(abstract_text)\\n\",\n",
        "        \"    return abstracts\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 5: Build vector store\\n\",\n",
        "        \"def build_vectorstore_from_articles(articles):\\n\",\n",
        "        \"    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\n\",\n",
        "        \"    texts = text_splitter.create_documents(articles)\\n\",\n",
        "        \"    embeddings = GoogleGenerativeAIEmbeddings(model=\\\"models/embedding-001\\\")\\n\",\n",
        "        \"    vectorstore = FAISS.from_documents(texts, embeddings)\\n\",\n",
        "        \"    return vectorstore\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 6: Create Gemini-based QA system\\n\",\n",
        "        \"def create_qa_chain(vectorstore):\\n\",\n",
        "        \"    llm = ChatGoogleGenerativeAI(model=\\\"gemini-1.5-pro\\\", temperature=0.2)\\n\",\n",
        "        \"    retriever = vectorstore.as_retriever()\\n\",\n",
        "        \"    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\\n\",\n",
        "        \"    return qa_chain\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 7: Ask your question\\n\",\n",
        "        \"def ask_health_question(query, qa_chain):\\n\",\n",
        "        \"    result = qa_chain(query)\\n\",\n",
        "        \"    print(\\\"\\\\n🩺 Answer:\\\\n\\\")\\n\",\n",
        "        \"    print(result[\\\"result\\\"])\\n\",\n",
        "        \"    print(\\\"\\\\n📚 Sources:\\\")\\n\",\n",
        "        \"    for i, doc in enumerate(result[\\\"source_documents\\\"]):\\n\",\n",
        "        \"        print(f\\\"\\\\nSource {i+1}:\\\\n{doc.page_content[:500]}...\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# ✅ Step 8: Run everything interactively\\n\",\n",
        "        \"if __name__ == \\\"__main__\\\":\\n\",\n",
        "        \"    user_query = input(\\\"💬 Enter your medical/healthcare question: \\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"    print(\\\"\\\\n🔍 Searching PubMed for related research...\\\")\\n\",\n",
        "        \"    articles = fetch_pubmed_articles(user_query, max_results=5)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    if not articles:\\n\",\n",
        "        \"        print(\\\"❌ No articles found on this topic. Try a different question.\\\")\\n\",\n",
        "        \"    else:\\n\",\n",
        "        \"        print(\\\"📚 Building knowledge base from PubMed articles...\\\")\\n\",\n",
        "        \"        vectorstore = build_vectorstore_from_articles(articles)\\n\",\n",
        "        \"\\n\",\n",
        "        \"        print(\\\"🤖 Connecting to Gemini for answer generation...\\\")\\n\",\n",
        "        \"        qa_chain = create_qa_chain(vectorstore)\\n\",\n",
        "        \"\\n\",\n",
        "        \"        ask_health_question(user_query, qa_chain)\\n\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"colab\": {\n",
        "          \"base_uri\": \"https://localhost:8080/\"\n",
        "        },\n",
        "        \"id\": \"-3TXjysvwbZo\",\n",
        "        \"outputId\": \"9552357b-8dcd-47d4-b961-1334711b859b\"\n",
        "      },\n",
        "      \"execution_count\": 11,\n",
        "      \"outputs\": [\n",
        "        {\n",
        "          \"output_type\": \"stream\",\n",
        "          \"name\": \"stdout\",\n",
        "          \"text\": [\n",
        "            \"💬 Enter your medical/healthcare question: Coronary artery disease mechanisms\\n\",\n",
        "            \"\\n\",\n",
        "            \"🔍 Searching PubMed for related research...\\n\",\n",
        "            \"📚 Building knowledge base from PubMed articles...\\n\",\n",
        "            \"🤖 Connecting to Gemini for answer generation...\\n\",\n",
        "            \"\\n\",\n",
        "            \"🩺 Answer:\\n\",\n",
        "            \"\\n\",\n",
        "            \"Excessive production of reactive oxygen species (ROS) contributes to endothelial dysfunction, inflammation, and vascular remodeling.  These pathological effects drive atherosclerosis, plaque formation, and thrombosis.\\n\",\n",
        "            \"\\n\",\n",
        "            \"📚 Sources:\\n\",\n",
        "            \"\\n\",\n",
        "            \"Source 1:\\n\",\n",
        "            \"Oxidative stress plays a pivotal role in the pathogenesis of atherosclerosis and \\n\",\n",
        "            \"coronary artery disease (CAD), with both beneficial and detrimental effects on \\n\",\n",
        "            \"cardiovascular health. On one hand, the excessive production of reactive oxygen \\n\",\n",
        "            \"species (ROS) contributes to endothelial dysfunction, inflammation, and vascular \\n\",\n",
        "            \"remodeling, which are central to the development and progression of CAD. These \\n\",\n",
        "            \"pathological effects drive key processes such as atherosclerosis, plaque \\n\",\n",
        "            \"formation, and thromb...\\n\",\n",
        "            \"\\n\",\n",
        "            \"Source 2:\\n\",\n",
        "            \"targeted therapies. The future directions for research aimed at harnessing the \\n\",\n",
        "            \"diagnostic and therapeutic potential of oxidative stress biomarkers are also \\n\",\n",
        "            \"discussed. Understanding the balance between the detrimental and beneficial \\n\",\n",
        "            \"effects of oxidative stress could lead to innovative approaches in the \\n\",\n",
        "            \"prevention and management of CAD....\\n\",\n",
        "            \"\\n\",\n",
        "            \"Source 3:\\n\",\n",
        "            \"response to prevent infections. Additionally, oxidative stress can stimulate \\n\",\n",
        "            \"cellular adaptation to stress, promote cell survival, and encourage \\n\",\n",
        "            \"angiogenesis, which helps form new blood vessels to improve blood flow. \\n\",\n",
        "            \"Oxidative stress also holds promise as a source of biomarkers that could aid in \\n\",\n",
        "            \"the diagnosis, prognosis, and monitoring of CAD. Specific oxidative markers, \\n\",\n",
        "            \"such as malondialdehyde (MDA), isoprostanes (isoP), ischemia-modified albumin, \\n\",\n",
        "            \"and antioxidant enzyme activity, have bee...\\n\",\n",
        "            \"\\n\",\n",
        "            \"Source 4:\\n\",\n",
        "            \"1. Antioxidants (Basel). 2025 Feb 26;14(3):275. doi: 10.3390/antiox14030275.\\n\",\n",
        "            \"\\n\",\n",
        "            \"The Dual Role of Oxidative Stress in Atherosclerosis and Coronary Artery \\n\",\n",
        "            \"Disease: Pathological Mechanisms and Diagnostic Potential.\\n\",\n",
        "            \"\\n\",\n",
        "            \"Myszko M(1), Bychowski J(1), Skrzydlewska E(2), Łuczaj W(2).\\n\",\n",
        "            \"\\n\",\n",
        "            \"Author information:\\n\",\n",
        "            \"(1)Department of Cardiology, Bialystok Regional Hospital, M. Skłodowskiej-Curie \\n\",\n",
        "            \"25, 15-950 Bialystok, Poland.\\n\",\n",
        "            \"(2)Department of Analytical Chemistry, Medical University of Bialystok, \\n\",\n",
        "            \"Mickiewicza 2d, 15-222...\\n\"\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ]
    }
  ]
}